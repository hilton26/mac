{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d09a589",
   "metadata": {},
   "source": [
    "### Towards Data Science\n",
    "#### Build and deploy your own AI chatbot to hundreds of users\n",
    "https://towardsdatascience.com/build-your-own-ai-chatbot-84d3b08e86f3\n",
    "\n",
    "### CollectedNotes.com\n",
    "#### Create a Stateful Chat AI in 10 minutes \n",
    "https://collectednotes.com/chai/create-a-stateful-chat-ai-in-10-minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990eb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install npu\n",
    "#npu in c:\\users\\hilton.netta\\anaconda3\\lib\\site-packages (0.3.900)\n",
    "#!pip install retry\n",
    "#retry in c:\\users\\hilton.netta\\anaconda3\\lib\\site-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092afd3e",
   "metadata": {},
   "source": [
    "To make things easier I put all the relevant code together in this GitHub repo: “Example Chai Bot”: https://github.com/tr416/example_chai_bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27a335",
   "metadata": {},
   "source": [
    "First we need a way to interact with GPT-J, our AI model. This is done via the FineTunedAPI class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ed2c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'npu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/qpcd_fbx3y18d0537n7lmhvm0000gn/T/ipykernel_19213/4065459197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFineTunedAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'npu'"
     ]
    }
   ],
   "source": [
    "import npu\n",
    "import retry\n",
    "\n",
    "class FineTunedAPI(object):\n",
    "  def __init__(self, temperature, rep_penalty):\n",
    "    API_TOKEN = \"<EMAIL dev@chai.ml for token>\"\n",
    "    npu.api(API_TOKEN, deployed=True)\n",
    "    self.temperature = temperature\n",
    "    self.rep_penalty = rep_penalty\n",
    "\n",
    "  @retry.retry(tries=3, backoff=2)\n",
    "  def request(self, data):\n",
    "    \"\"\"\n",
    "    input: data is a string, the message we want a response to.\n",
    "    output: (string) reponse from the model.\n",
    "    \"\"\"\n",
    "    data = data[-2048:]\n",
    "    kwargs = {\n",
    "    \"remove_input\": True,  # whether to return your input\n",
    "    \"do_sample\": True,  # important to get realistic sentences and not just MLE\n",
    "    \"temperature\": self.temperature,\n",
    "    \"response_length\": 32,  # how many response tokens to generate\n",
    "    \"repetition_penalty\": self.rep_penalty,  # 1 is the default\n",
    "    \"eos_token_id\": 198,  # this is \\n\n",
    "    }\n",
    "    model_id = \"<EMAIL dev@chai.ml for model_id>\"\n",
    "    output = npu.predict(model_id, [data], kwargs)\n",
    "    resp = output[0][\"generated_text\"]\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf4e38",
   "metadata": {},
   "source": [
    "Notice how simple it is: there is just a “request” function which we have to give a string to (this is the message we want a response to). In this API we set the repetition penalty, temperature, and response length. Just as in the playground example earlier.<br><br>\n",
    "Great, now that we can query GPT-J, we just need to take that, and turn it into a chatbot. We’re going to want to give the bot a prompt, and an example conversation. Just like in the playground. The ChatAI class below does this for us in the __init__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bda0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ChatAI(object):                                                           \n",
    "    def __init__(self, model):                                                  \n",
    "        self.bot_name = \"Chris\"                                                 \n",
    "        self.chat_history = [                                                   \n",
    "            \"User: hi\",                                                         \n",
    "            \"{bot_name}: Hello there! I'm Chris. I work as a coder but I love rock climbing. What about you?\",\n",
    "            \"User: hi chris, nice to meet you\",                                 \n",
    "            \"{bot_name}: Nice to meet you! How are you feeling today?\",         \n",
    "        ]                                                                       \n",
    "        self.max_history = 104                                                  \n",
    "        self.model = model                                                      \n",
    "        self.prompt = \"Chris is a 30 year old male software developer.  He is also a Christian and likes to give people good advice.  Chris speaks softly and politely. Chris is friendly.  Chris wants to be User's friend.\"\n",
    "                                          \n",
    "                                                                                \n",
    "    def get_resp(self, input_message):                                          \n",
    "        \"\"\"\"                                                                    \n",
    "        When we send a request the chat_history has to look like below for      \n",
    "        the model to know it is the bot speaking:                               \n",
    "        User: input_message                                                     \n",
    "        {bot_name}:                                                             \n",
    "        \"\"\"                                                                     \n",
    "        self._update_chat_history(message=input_message, sender=\"User\")         \n",
    "        self._update_chat_history(message=\"\", sender=\"{bot_name}\")              \n",
    "                                                                                \n",
    "        request = self._prepare_request()                                       \n",
    "        raw_response = self.model.request(request)                              \n",
    "        formatted_response = self._format_model_response(raw_response)          \n",
    "                                                                                \n",
    "        self.chat_history[-1] += formatted_response                             \n",
    "        return formatted_response    \n",
    "      \n",
    "    def _prepare_request(self):                                                                    \n",
    "        formatted_chat_history = self.chat_history[-self.max_history :]                            \n",
    "        formatted_chat_history = \"\\n\".join(formatted_chat_history)                                 \n",
    "        request = self.prompt + formatted_chat_history                                             \n",
    "        request = request.replace(\"{bot_name}\", self.bot_name)                                     \n",
    "        return request                                                                             \n",
    "                                                                                                   \n",
    "    def _format_model_response(self, resp):                                                        \n",
    "        partial_chat = re.split(                                                                   \n",
    "            \"User|{lower_bot_name}:|{bot_name}:|\\n|\\$\".format(                  \n",
    "                lower_bot_name=self.bot_name.lower(), bot_name=self.bot_name    \n",
    "            ),                                                                  \n",
    "            resp,                                                               \n",
    "        )[0]                                                                    \n",
    "        partial_chat = partial_chat.strip(\" \")                                  \n",
    "        if len(partial_chat) == 0:                                              \n",
    "            partial_chat = \"...\"                                                \n",
    "        return partial_chat                                                     \n",
    "                                                                                \n",
    "    def _update_chat_history(self, message, sender):                            \n",
    "        cleaned_message = message.strip(\" \")                                    \n",
    "        self.chat_history += [f\"{sender}: {cleaned_message}\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a1d5e",
   "metadata": {},
   "source": [
    "You can safely ignore all the code except from the __init__ where we define the personality of the bot by giving it a prompt and a conversation history.<br><br>\n",
    "Perfect, now we can package the two classes above together nicely and we will have a chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d952d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U chaipy\n",
    "#Successfully installed chaipy-0.3.12 halo-0.0.31 log-symbols-0.0.14 segno-1.4.1 spinners-0.0.24 termcolor-1.1.0\n",
    "#!pip install gpt\n",
    "#Successfully installed gpt-0.2\n",
    "#!pip install utils\n",
    "#Successfully installed utils-1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ae2970",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatAI' from 'gpt' (C:\\Users\\hilton.netta\\Anaconda3\\lib\\site-packages\\gpt\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-35fc8fe1be81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchai_py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChaiBot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgpt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChatAI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFineTunedAPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ChatAI' from 'gpt' (C:\\Users\\hilton.netta\\Anaconda3\\lib\\site-packages\\gpt\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from chai_py import ChaiBot, Update\n",
    "from gpt import ChatAI, FineTunedAPI\n",
    "from utils import truncate\n",
    "\n",
    "\n",
    "class Replica(ChaiBot):\n",
    "    def setup(self):\n",
    "        self.logger.info(\"Setting up...\")\n",
    "        self.model = ChatAI(FineTunedAPI(temp=0.5, rep_penalty=1.15))\n",
    "        self.max_len = 100\n",
    "\n",
    "    async def on_message(self, update):\n",
    "        return self.respond(update.latest_message.text)\n",
    "\n",
    "    def respond(self, message):\n",
    "        if message == \"__first\":\n",
    "            output = \"Hi there! I'm Chris, a software developer from London. What about yourself?\"\n",
    "        else:\n",
    "            output = self.model.get_resp(message)\n",
    "            if len(output) > self.max_len:\n",
    "                output = truncate(output, self.max_len)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef15ac",
   "metadata": {},
   "source": [
    "How to fix the error : \"cannot import name 'GPT' from \"gpt\"\n",
    "https://stackoverflow.com/questions/68444704/how-to-fix-the-error-cannot-import-name-gpt-from-gpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
