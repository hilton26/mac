{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9556c10c",
   "metadata": {},
   "source": [
    "Copy pandas dataframe to excel using openpyxl<br>\n",
    "https://stackoverflow.com/questions/36657288/copy-pandas-dataframe-to-excel-using-openpyxl\n",
    "\n",
    "How to delete one or more rows in excel using Openpyxl? Deleting all rows, Method 2<br>\n",
    "https://www.geeksforgeeks.org/how-to-delete-one-or-more-rows-in-excel-using-openpyxl/\n",
    "\n",
    "How to remove commas from ALL the column in pandas at once\n",
    "https://stackoverflow.com/questions/56947333/how-to-remove-commas-from-all-the-column-in-pandas-at-once\n",
    "\n",
    "Convert number strings with commas in pandas DataFrame to float<br>\n",
    "https://stackoverflow.com/questions/22137723/convert-number-strings-with-commas-in-pandas-dataframe-to-float\n",
    "\n",
    "Extracting date from a string in Python<br>\n",
    "https://stackoverflow.com/questions/3276180/extracting-date-from-a-string-in-python\n",
    "\n",
    "How to utilise the date_parser parameter of pandas.read_csv()<br>\n",
    "https://stackoverflow.com/questions/60349071/how-to-utilise-the-date-parser-parameter-of-pandas-read-csv\n",
    "\n",
    "Python date & time conversion CheatSheet<br>\n",
    "https://dev.to/maikomiyazaki/python-date-time-conversion-cheatsheet-3m69#:~:text=To%20convert%20float%20of%20timestamp,argument%20and%20returns%20DateTime%20object.\n",
    "\n",
    "A Complete Guide to Using Progress Bars in Python<br>\n",
    "https://towardsdatascience.com/a-complete-guide-to-using-progress-bars-in-python-aa7f4130cda8\n",
    "\n",
    "How do I list all files of a directory?<br>\n",
    "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "Make the Most Out of your pandas.read_csv()<br>\n",
    "https://medium.com/analytics-vidhya/make-the-most-out-of-your-pandas-read-csv-1531c71893b5\n",
    "\n",
    "Converting column into proper timestamp using pandas read_csv<br>\n",
    "https://stackoverflow.com/questions/42324119/converting-column-into-proper-timestamp-using-pandas-read-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2402350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- -1.2 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time # to calculate durations of runs\n",
    "start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm, notebook # notebook version of tqdm\n",
    "#\" ...any time you see a loop somewhere in your code in you can simply wrap it in either tdqm() or tqdm_notebook() in Jupyter\" \n",
    "\n",
    "print(f\"--- {round(start_time - time.time(),1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8cf1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'str'>\n",
      "--- -0.1 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# set variables and make dataframes out of the Eagle fund holdings and deltas .CSV workbooks\n",
    "start_time = time.time()\n",
    "\n",
    "# set path to the Derv.calc file\n",
    "fH = r'C:\\Users\\hilton.netta\\Downloads\\Portfolio Analytics Report - New.csv' # the Eagle holdings spreadsheet\n",
    "fD = r'C:\\Users\\hilton.netta\\Downloads\\Derivative Exposure.csv' # the Eagle delats spreadsheet\n",
    "fDx = Path('//pim-cpt-fs/profile$/PProfile Operations/Legal/Legal and Compliance Framework/Hilton/W/Derv.xlsx') # Derv.xlsx\n",
    "pthSave = r'C:\\Users\\hilton.netta\\Documents\\DervFiles' # path to were the files will be saved\n",
    "\n",
    "# create dataframes of the Eagle holdings and deltas spreadsheets, using \"thousands = ','\" to deal with strings\n",
    "wbD = pd.read_csv(fD, thousands=',') # holdings from Eagle\n",
    "\n",
    "# convert string with commas to flaots and convert time strings in column AJ to \"dd/mm/yyyy\" format\n",
    "z = lambda t: datetime.strptime(t) # creating a function to format the two important date columns P and AJ\n",
    "wbH = pd.read_csv(fH, thousands=',') # holdings from Eagle\n",
    "#https://stackoverflow.com/questions/60349071/how-to-utilise-the-date-parser-parameter-of-pandas-read-csv\n",
    "print(type(wbH['i Position Effective Date'][0]),type(wbH['Maturity Date'][0]))\n",
    "\n",
    "print(f\"--- {round(start_time - time.time(),1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150e1064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/05/2022 31/03/2046\n"
     ]
    }
   ],
   "source": [
    "print(wbH['i Position Effective Date'][7], wbH['Maturity Date'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7adbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-23 00:00:00 2028-05-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.strptime(wbH['i Position Effective Date'][0],\"%d/%m/%Y\"),datetime.strptime(wbH['Maturity Date'][0],\"%d/%m/%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c62d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '' does not match format '%d/%m/%Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bae3c2b03efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# convert string with commas to flaots and convert time strings in column AJ to \"dd/mm/yyyy\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%d/%m/%Y\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# creating a function to format the two important date columns P and AJ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mwbH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'i Position Effective Date'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Maturity Date'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# holdings from Eagle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#https://stackoverflow.com/questions/60349071/how-to-utilise-the-date-parser-parameter-of-pandas-read-csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._apply_converter\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bae3c2b03efd>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# convert string with commas to flaots and convert time strings in column AJ to \"dd/mm/yyyy\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%d/%m/%Y\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# creating a function to format the two important date columns P and AJ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mwbH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'i Position Effective Date'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Maturity Date'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# holdings from Eagle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#https://stackoverflow.com/questions/60349071/how-to-utilise-the-date-parser-parameter-of-pandas-read-csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[0;32m    350\u001b[0m                          (data_string, format))\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data '' does not match format '%d/%m/%Y'"
     ]
    }
   ],
   "source": [
    "# set variables and make dataframes out of the Eagle fund holdings and deltas .CSV workbooks\n",
    "start_time = time.time()\n",
    "\n",
    "# set path to the Derv.calc file\n",
    "fH = r'C:\\Users\\hilton.netta\\Downloads\\Portfolio Analytics Report - New.csv' # the Eagle holdings spreadsheet\n",
    "fD = r'C:\\Users\\hilton.netta\\Downloads\\Derivative Exposure.csv' # the Eagle delats spreadsheet\n",
    "fDx = Path('//pim-cpt-fs/profile$/PProfile Operations/Legal/Legal and Compliance Framework/Hilton/W/Derv.xlsx') # Derv.xlsx\n",
    "pthSave = r'C:\\Users\\hilton.netta\\Documents\\DervFiles' # path to were the files will be saved\n",
    "\n",
    "# create dataframes of the Eagle holdings and deltas spreadsheets, using \"thousands = ','\" to deal with strings\n",
    "wbD = pd.read_csv(fD, thousands=',') # holdings from Eagle\n",
    "\n",
    "# convert string with commas to flaots and convert time strings in column AJ to \"dd/mm/yyyy\" format\n",
    "z = lambda t: datetime.strptime(t,\"%d/%m/%Y\") # creating a function to format the two important date columns P and AJ\n",
    "wbH = pd.read_csv(fH, thousands=',', converters = {'i Position Effective Date':z,'Maturity Date':z}) # holdings from Eagle\n",
    "#https://stackoverflow.com/questions/60349071/how-to-utilise-the-date-parser-parameter-of-pandas-read-csv\n",
    "\n",
    "# get the report date and list of fund names\n",
    "rptDate = wbH.iloc[0, 35].strftime(\"%#d%b%Y\") # 1st row 36th column has the report date\n",
    "fnames = wbH['Entity Name'].unique() # list of the fund names in the holdings file\n",
    "\n",
    "print(f\"--- {round(time.time() - start_time,1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "107f8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/05/2022 15/05/2028\n"
     ]
    }
   ],
   "source": [
    "print(wbH['i Position Effective Date'][0],wbH['Maturity Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c75700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318a7ea0eada43958c3b7993363429fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.4 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# change the '% of Total Market Value\" column (N) to the fund-specific % of NAV based on 'Sum of Market Value Income' column (M)\n",
    "start_time = time.time()\n",
    "\n",
    "navs = wbH.groupby('Entity ID')['Sum of Market Value Income'].sum() # (column N) this has type 'pandas.core.series.Series'\n",
    "nav = navs.to_dict() # nav series changed to dictionary to make it lookupable\n",
    "\n",
    "# recalc the '% of Total Market Value' column per fund and ...\n",
    "newTMV=[]\n",
    "for i, row in notebook.tqdm(wbH.iterrows()):\n",
    "    if nav[row['Entity ID']] == 0:\n",
    "        fndpct = 1\n",
    "    else:\n",
    "        fndpct = row['Sum of Market Value Income'] / nav[row['Entity ID']] * 100\n",
    "    \n",
    "    newTMV.append(fndpct)\n",
    "    \n",
    "# ... replace the '% of Total Market Value' with the values in the new list\n",
    "wbH['% of Total Market Value'] = newTMV # wbH['% of Total Market Value'].sum(), check, should equal number of funds\n",
    "\n",
    "\n",
    "\n",
    "# change the 'Current Exposure %\" column (AI) to the fund-specific current % of NAV based on \"Current Exposure\" column (AH)\n",
    "\n",
    "# recalc the '% of Total Market Value' column per fund and ...\n",
    "newCEp=[] # new Current Exposure % column\n",
    "for i, row in notebook.tqdm(wbH.iterrows()):\n",
    "    if nav[row['Entity ID']] == 0:\n",
    "        currentexposurepct = 1\n",
    "    else:\n",
    "        currentexposurepct = row['Current Exposure'] / nav[row['Entity ID']] * 100\n",
    "    \n",
    "    newCEp.append(currentexposurepct )\n",
    "    \n",
    "# ... replace the '% of Total Market Value' with the values in the new list\n",
    "wbH['Current Exposure %'] = newCEp # wbH['% of Total Market Value'].sum(), check, should equal number of funds\n",
    "\n",
    "print(f\"--- {round(time.time() - start_time,1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219b31e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fnames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e67700db464c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mwbDervCalc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfDx\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# load the derivative calculation file as an openpyxl workbook (afresh)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fnames' is not defined"
     ]
    }
   ],
   "source": [
    "# copy the holdings and deltas data per fund to the Derv.xslx calculation sheet\n",
    "start_time = time.time()\n",
    "\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "for fname in notebook.tqdm(fnames):\n",
    "    wbDervCalc = load_workbook(fDx)          # load the derivative calculation file as an openpyxl workbook (afresh)\n",
    "    \n",
    "    # populate the 'Data' sheet in Derv.calc ... \n",
    "    wsH = wbDervCalc['Data'] # the 'Data' sheet of Derv.xslx\n",
    "    fundCode = wbH[wbH['Entity Name'] == fname].iloc[0, 36] # fund code to be included in name of saved file\n",
    "    rows = dataframe_to_rows(wbH[wbH['Entity Name'] == fname], index = False)\n",
    "    for r_idx, row in enumerate(rows, 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "             wsH.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # ... then populate the 'Deltas' sheet in Derv.calc ...\n",
    "    wsD = wbDervCalc['Deltas'] # the 'Deltas' sheet of Derv.xlsx\n",
    "    rows = dataframe_to_rows(wbD[wbD['Entity Name'] == fname], index = False)\n",
    "    for r_idx, row in enumerate(rows, 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "             wsD.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # ... and then save the derivative calculation sheet\n",
    "    wbDervCalc.save(os.path.join(pthSave,f'{fundCode} Derv Calc {rptDate}.xlsx')) # save the file\n",
    "\n",
    "wbDervCalc.close() # close the laoded workbook\n",
    "\n",
    "print(f\"--- {round(time.time() - start_time,1)} seconds ---\") # for 77 files stored locally, this takes around 2Â½ to 3 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fe9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------ ========= ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f62ca178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-16 00:00:00 <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "oppxl1 = wbH.iloc[0, 35]\n",
    "oppxl2 = wbH.iloc[5, 35]\n",
    "print(oppxl1, type(oppxl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854a1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-16 00:00:00 <class 'pandas._libs.tslibs.timestamps.Timestamp'> 2048-02-28 00:00:00 <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "tre = r'\\\\pim-cpt-statpro\\SPC_602\\Client\\Profiles\\PIM\\Exports\\PABS Derv Calc 16May2022.xlsx'\n",
    "\n",
    "wbC = pd.read_excel(tre, sheet_name = 'Data')\n",
    "\n",
    "print(wbC.iloc[1,35],type(wbC.iloc[1,35]), wbC.iloc[1,15],type(wbC.iloc[1,15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc0efa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6fc9b0bcb5cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwbH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mpandas\\_libs\\tslibs\\timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`"
     ]
    }
   ],
   "source": [
    "wbH.iloc[1,35]+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AN ITERATION OF THE LOOP\n",
    "\n",
    "fname = 'PIM Balanced'\n",
    "\n",
    "wbDervCalc = load_workbook(fDx)          # load the derivative calculation file as an openpyxl workbook afresh\n",
    "# populate the 'Data' sheet in Derv.calc ... \n",
    "wsH = wbDervCalc['Data'] # the 'Data' sheet of Derv.xslx\n",
    "subsetH = wbH[wbH['Entity Name'] == fname]\n",
    "fundCode = subsetH.iloc[0, 36] # fund code to be included in name of saved file\n",
    "rows = dataframe_to_rows(subsetH, index = False)\n",
    "for r_idx, row in enumerate(rows, 1):\n",
    "    for c_idx, value in enumerate(row, 1):\n",
    "         wsH.cell(row=r_idx, column=c_idx, value=value)\n",
    "            \n",
    "# ... then populate the 'Deltas' sheet in Derv.calc ...\n",
    "wsD = wbDervCalc['Deltas'] # the 'Deltas' sheet of Derv.xlsx\n",
    "subsetD = wbD[wbD['Entity Name'] == fname]\n",
    "rows = dataframe_to_rows(subsetD, index = False)\n",
    "for r_idx, row in enumerate(rows, 1):\n",
    "    for c_idx, value in enumerate(row, 1):\n",
    "         wsD.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "# ... and then save the derivative calculation sheet\n",
    "wbDervCalc.save(os.path.join(pthSave,f'{fundCode} Derv Calc {rptDate}.xlsx')) # save the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
