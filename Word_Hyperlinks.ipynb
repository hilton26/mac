{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68178e32",
   "metadata": {},
   "source": [
    "### Python extracts links (hyperlink) and text from word/docx\n",
    "https://stdworkflow.com/441/python-extracts-links-hyperlink-and-text-from-word-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc262ea",
   "metadata": {},
   "source": [
    "How to index into a dictionary?<br>\n",
    "https://stackoverflow.com/questions/4326658/how-to-index-into-a-dictionary\n",
    "\n",
    "Python typeerror: ‘int’ object is not iterable Solution<br>\n",
    "https://careerkarma.com/blog/python-typeerror-int-object-is-not-iterable/#:~:text=TypeErrors%20are%20a%20common%20type,iterable%20rather%20than%20a%20number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90cfdd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries ###\n",
    "import zipfile\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "from docx import Document\n",
    "from os.path import basename\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "### Constants ###\n",
    "pth = 'J:\\\\PProfile Operations\\\\Legal\\\\Legal and Compliance Framework\\\\Hilton\\\\Client Mandates\\\\'\n",
    "\n",
    "\n",
    "filename = pth + 'POIF Rules.docx'\n",
    "\n",
    "### Functions ###\n",
    "def get_linked_text(soup):\n",
    "\n",
    "    links = []\n",
    "\n",
    "    # This kind of link has a corresponding URL in the _rel file.\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        # try/except because some hyperlinks have no id.\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # This kind does not.\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        # They're identified by the word HYPERLINK\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            # Get the URL. Probably.\n",
    "            url = tag.text.split('\"')[1]\n",
    "\n",
    "            # The actual linked text is stored nearby tags.\n",
    "            # Loop through the siblings starting here.\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                # Text comes in <t> tags.\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None:\n",
    "                    # Ones that have text in them.\n",
    "                    if maybe_text.text.strip() != \"\":\n",
    "                        text += maybe_text.text.strip()\n",
    "\n",
    "                # Links end with <w:fldChar w:fldCharType=\"end\" />.\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None:\n",
    "                    if maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                        break\n",
    "\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "\n",
    "    return links\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #file_name = filename\n",
    "    #archive = zipfile.ZipFile(file_name, \"r\")\n",
    "    #file_data = archive.read(\"word/document.xml\")\n",
    "    #doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "    #linked_text = get_linked_text(doc_soup)\n",
    "    #print(linked_text)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   linked_text = get_linked_text(BeautifulSoup(zipfile.ZipFile(filename, \"r\").read(\"word/document.xml\"), \"xml\"))\n",
    "\n",
    "#print(linked_text)\n",
    "\n",
    "def links(filename):\n",
    "    return get_linked_text(BeautifulSoup(zipfile.ZipFile(filename, \"r\").read(\"word/document.xml\"), \"xml\"))  \n",
    "    #print(get_linked_text(BeautifulSoup(zipfile.ZipFile(filename, \"r\").read(\"word/document.xml\"), \"xml\")))\n",
    "    \n",
    "def get_hyperlinks(text,filename):\n",
    "    linked_text = links(filename)\n",
    "    hlink = []\n",
    "    for i in range(len(linked_text)):\n",
    "        m = linked_text[i][list(linked_text[i])[1]]\n",
    "    if text in m:\n",
    "        hlink.append(m)\n",
    "    return hlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20300990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hyperlinks('Segregated Clients',pth + 'POIF Rules.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b482a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
